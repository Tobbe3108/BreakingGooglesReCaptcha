<h1 id="breaking-googles-image-based-recaptcha-v2">Breaking Googleâ€™s Image-based reCaptcha v2</h1>

<p>I dette projekt har vi valgt at vi vil forsÃ¸ge at bryde Googleâ€™s reCAPTCHA v2 med et neuralt netvÃ¦rk.
reCAPTCHA tester den menneskelige evne til at genkende objekter pÃ¥ billeder, derfor er det en oplagt opgave at forsÃ¸ge at lÃ¸se med et neuralt netvÃ¦rk.
Mange virksomheder er for lÃ¦ngst skiftet vÃ¦k fra reCAPTCHA v2 til andre bedre alternativer, men man stÃ¸der stadig ofte pÃ¥ denne reCAPTCHA.</p>

<h2 id="collect-data">Collect Data</h2>
<p>Da vi skulle finde et datasÃ¦t til at trÃ¦ne vores model, prÃ¸vede vi at google â€œrecaptcha datasÃ¦tâ€, for at se om der allerede var nogen der havde lavet et. 3. link fÃ¸rte til en github repository kaldet â€œrecaptcha datasetâ€ (https://github.com/brian-the-dev/recaptcha-dataset).
Efter et nÃ¦rmere kig pÃ¥ billederne i datasÃ¦ttet, viste det sig at vÃ¦re et rigtig godt udgangspunkt for vores projekt.
DatasÃ¦ttet indeholder mere end 10.000 billeder, fordelt pÃ¥ 11 kategorier sÃ¥som â€œBicycleâ€, â€œCrosswalkâ€ og â€œTraffic Lightâ€, som er det man ofte skal identificere i googles recaptcha.
Desuden er billederne allerede skaleret ned og beskÃ¥ret til et 1:1 format, pÃ¥ samme mÃ¥de som i recaptchaen. DatasÃ¦ttet er ogsÃ¥ frigivet under en MIT licens, sÃ¥ vi uden bekymringer kan gÃ¸re brug af det.</p>

<p>En potentiel udfordring ved datasÃ¦ttet er dog, at flere af billederne passer ind i flere kategorier.
Fx er der mange af billederne af â€œTraffic Lightsâ€ som ogsÃ¥ indeholder en â€œCrosswalkâ€.
DatasÃ¦ttet er dog lavet sÃ¥dan, at hvert billede kun er i en kategori, og tager ikke hÃ¸jde for om det passer ind flere steder.</p>

<p>Efter at have downloadet datasÃ¦ttet valgte vi midlertidigt at trimme stÃ¸rstedelen af billederne vÃ¦k, sÃ¥ der kun var en lille mÃ¦ngde billeder i hver kategori.
Det gjorde vi for at gÃ¸re det hurtigere at trÃ¦ne modellen under de indledende kode-iterationer.
FÃ¸rst da vi var overbeviste om at vi havde styr pÃ¥ koden, trÃ¦nede vi modellen op imod det fulde datasÃ¦t.</p>

<h2 id="prepare-the-data">Prepare the data</h2>
<p>###OpsÃ¦tning af datablock
Forberedelsen af dataâ€™en sker i vores datablock.
Vores block er en ImageBlock som er MultiCategoryBlock. Det vil altsÃ¥ sige at vi vil have modellen har mulighed for at kunne genkende mere end et label pÃ¥ billederne.</p>

<p>get_items bliver hentet gennem fastaiâ€™s get_image_files metode.
get_y bliver sat gennem egen metode get_y som sÃ¦tter parent_label til billedet.
splitter bliver sat efter default parametre hvor der er en valid_pct pÃ¥ 0.2 og seed er 42.
item_tfms sÃ¦ttes til 128 og min_scale til 0.35. Dette gÃ¸r at billederne har en minimumsstÃ¸rrelse pÃ¥ 128 pixels med en scale pÃ¥ minimum 0.35</p>

<p>NÃ¥r modellens datablock bliver instansieret sÃ¦ttes der ikke en batch_tfms.
I dette tilfÃ¦lde betyder det at fastai automatisk fÃ¥r lov til at normalisere dataâ€™en udfra en enkelt batch af dataâ€™en.</p>

<p>###DÃ¥rlige billeder
I det datasÃ¦t som der er blevet fundet er der ikke blevet fjernet dÃ¥rlige billeder.
Dette skyldes bl.a. at en reCaptcha er sat op til at kunne have dÃ¥rlige billeder, hvor det som der bliver spurgt efter ikke nÃ¸dvendigvis er i fokus eller god kvalitet.
###Resultat
Som det kan ses i opsÃ¦tning af datablockâ€™en, sÃ¥ er de fleste vÃ¦rdier â€œdefaultâ€ vÃ¦rdier, som fastai anbefaler man benytter i fÃ¸rste opsÃ¦tning.
Dette har vi ikke ydeligere modificeret pÃ¥ eftersom at vores baseline model har resulteret i en accuracy pÃ¥ op til 94%.</p>

<h2 id="choose-the-model">Choose the model</h2>
<p>Et konventionelt neural networks network design har hver enkelte node forbundet til alle noder finding in i alle node i nÃ¦ste lag.
Dette gÃ¸r at backprobergationen selv skal finde mÃ¸nstre og segmentere datagen.
Med et Convolutional Neural Networks tvinger man netvÃ¦rket til at kun at kigge pÃ¥ mindre segmenter i stedet for selv at gÃ¸re det.
Med Convolutional Neural Networks findes der mange strategier man kan bruge til en neural Network design sÃ¥ som LeNet-5, AlexNet og VGG-16.
I vores projekt har vi brugt den indbygget cnn learner for multi category block.
Vi bruger Multi Category Block da det var et krav men ogsÃ¥ lader os benytte vores ressourcer bedre da vi kan tjekke hvad Recapture spÃ¸rg efter samtidig med billederne bliver labeled.</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_1.png â€œâ€)</p>

<p>Mens vi undersÃ¸gte hvordan vi optimerer vores model valgte vi at lave et mindre datasÃ¦t end havde vi oprindeligt fandt. 
Dette gjorde at vi kunne iterere hurtigere da det tog kortere tid at trÃ¦ne nÃ¥r vi laver Ã¦ndringer i vores lÃ¦rings parameter.
Ulemper med denne tilgang er tendenser i vores trÃ¦ning sÃ¦t bliver forstÃ¦rket fx de fleste modeller vi har trÃ¦net har ikke haft multi category labels pÃ¥ billederne men derimod kun et label.
Dette gjorde hvis der var en bro og en bil pÃ¥ samme billede, ville den kun vÃ¦lge den ene frem for begge. Mere om dette under Prediction afsnittet.
Vi er endt med en model det svare med Multi Category Block og bruger fastaiâ€™s indbyggede cnn lerner, og for hurtig iteration har vi et mindre datasÃ¦t.</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_2.png â€œâ€)</p>

<h2 id="train-your-machine-model">Train your machine model</h2>
<p>Med dataene og fremgangsmÃ¥den redegjort for, er vores projekt i trÃ¦ningsfasen.
Her fÃ¸lger vi trÃ¦ningen igennem vores udvalgte data, og validerer gennem fasens epochs hvilke antagelser som modellen laver, og hvilke der ender ud med at vÃ¦re korrekte.</p>

<p>I datasamlingen fortalte vi om vores opdeling af vores datagrundlag og en test sample, som er trimmet ned for at tillade at den indledende trÃ¦ning forlÃ¸ber hurtigt.
Dette gÃ¸r at vi kan opbygge vores optimale indstillinger for trÃ¦ningen og klargÃ¸re til det komplette datagrundlag.</p>

<p>Vi kan forsÃ¸ge os med at tilfÃ¸je flere kÃ¸rsler i form af Epochs for at finde ud af hvilken kvantitet af trÃ¦ning der giver os umiddelbart den bedste nÃ¸jagtighed.
I vores trimmede dataset kÃ¸rer Epochs hurtigt, og giver os et indblik i hvilke resultater vi kan forvente.</p>

<p>I trÃ¦ningen eksperimenterer vi ogsÃ¥ med forskellige Resnets for at gÃ¸re trÃ¦ningen hurtigere, hvilket baner vejen for en hurtig opstart i de fÃ¸rste Epochs, som sÃ¥ tilfÃ¸jer de mest komplekse layers tilbage i de senere Epochs for at opbygge et grundlag for modellen og validere den til sidst.
Vi valgte at gÃ¥ med Resnet50, da vi havde stÃ¸rst accuracy med testen undervejs og havde mest erfaring med denne mÃ¦ngde.</p>

<p>Det giver ogsÃ¥ mening at effektuere trÃ¦ningen ved hjÃ¦lp af CNN learnerâ€™ens Learning Rate Find metode.
Dette giver et indblik i hvordan CNN bedst kan lÃ¦rer ud fra vores data pÃ¥ den mest ressourceeffektive mÃ¥de.</p>

<h2 id="evaluation">Evaluation</h2>
<p><a href="/BreakingGooglesRecaptcha/2021/05/25/Captcha-Baseline-Model.html/">Baseline model notebook</a></p>

<p>For ikke at skulle trÃ¦ne modellen pÃ¥ alle 10.000 billeder hver gang da det taget lang tid har vi lavet et datasÃ¦t der tager 20 billeder fra hver kategori.
Dette datasÃ¦t har vi efterfÃ¸lgende brugt til vores aktive udvikling.
NÃ¥r vi har haft en ny model iteration vi gerne ville teste helt, har vi skiftet tilbage til det oprindelige datasÃ¦t med 10.000 billeder.
Vi har oplevet at dette lille datasÃ¦t har vÃ¦ret tilstrÃ¦kkeligt til at give en indikation af hvor godt modellen er.</p>

<p>I vores DataBlock har vi benyttet en RandomSplitter der udvÃ¦lger 0.2 procent af billederne som vores valideringssÃ¦t.
Billederne bliver ogsÃ¥ alle kÃ¸rt igennem en RandomResizedCrop der gÃ¸r at vi fÃ¥r mere variation i vores data.</p>

<p>Vi har benyttet den indbyggede leaning rate finder (Lr_find()) til at finde den optimale learning rate for vores learner.
Lr_find() metoden kÃ¸re et par iterationer med en meget lille learning rate.
For hvert mini-batch bliver der skruet en lille smule op for learning raten indtil den til sidst er meget hÃ¸j.
Iterationernes loss bliver sammenlignet med den valgte learning rate og den bedste bliver valgt.</p>

<p>Da vi trÃ¦nede denne model fÃ¸rste gang med ovenstÃ¥ende udgangspunkt, blev vi meget overraskede over at vores model opnÃ¥ede en accuracy pÃ¥ 94%.
Da vi testede modellen med et billede den aldrig have set fÃ¸r blev vi lige sÃ¥ overraskede.
Modellen var 99.6% sikker pÃ¥ at der er bro i det billede, men den var kun 0.07% sikker pÃ¥ at der var en bil ogsÃ¥ selv om det er meget tydeligt at se at der bÃ¥de er biler og en bro.
Dette betyder at vi har lavet en model der er rigtig god til at finde en enkelt ting i et billede men altsÃ¥ ikke flere ting som vores model skal kunne.</p>

<p>Vi har alligevel valgt at beholde denne model som vores baseline model pÃ¥ trods af dette da den giver et godt udgangspunkt vi kan teste vores fremtidige iterationer op imod.</p>

<h2 id="tuning--optimisation">Tuning / Optimisation</h2>
<p><a href="/BreakingGooglesRecaptcha/2021/05/25/Captcha-MixUp.html/">MixUp notebook</a><br />
<a href="/BreakingGooglesRecaptcha/2021/05/25/Captcha-LabelSmoothing.html/">Label Smoothing v1 notebook</a><br />
<a href="/BreakingGooglesRecaptcha/2021/05/25/Captcha-Label-Smoothing-v2.html/">Label Smoothing v2 notebook</a><br />
<a href="/BreakingGooglesRecaptcha/2021/05/25/Captcha-Label-Correction.html/">Label Correction notebook</a></p>

<p>Med vores nyoprettet baseline model, justerer vi vores forskellige parameter for at opnÃ¥ det bedste resultat.
For at sikre imod at vores model blev for sikker og â€œoverfittedâ€, brugte vi Label Smoothing undervejs i trÃ¦ningen for at gÃ¸re modellen mere modtagelig over for et mere nuanceret gÃ¦t i sidste ende, sÃ¥dan at modellen kan vÃ¦re bedre til at gÃ¦tte pÃ¥ andre mulige resultater i de billeder, som vi skal genkende.
Dette er rigtig nyttigt, til billeder som indeholder fx biler og broer, hvor at begge klassifikationer er korrekte, og gÃ¸r at vi kan validere om modellen tror at en bestemt kategori findes pÃ¥ billedet.
NÃ¥r reCAPTCHA sÃ¥ spÃ¸rger efter alle bjergene pÃ¥ en rÃ¦kke billeder, sÃ¥ er det ikke et problem at der er en bil pÃ¥ nogle af billederne.
Vores model kan stadig vÃ¦lge at kigge efter den specifikke kategori, beregner hvorvidt der er en sandsynlighed for at et billede indeholder et bjerg og kan pÃ¥ den mÃ¥de stadig svare rigtig.
Dette krÃ¦ver dog bare at vi specificerer et threshold for hvor lav sandsynlighed der skulle accepteres.</p>

<p>Til sidst i vores forsÃ¸g pÃ¥ at optimere vores model, tilfÃ¸jede vi en Mixup Callback metode til vores learner for at give bedre nÃ¸jagtighed pÃ¥ vores billedgenkendelse.
Med Mixup kan vi blande vores billeder sammen for at billederne skal genkendes ind over hinanden i et forsÃ¸g pÃ¥ at gÃ¸re learneren Ã¥ben over for tvetydighed i vores data.
NÃ¥r vores learner stÃ¸der pÃ¥ et billede der er opbygget af 20 % biler og 80 % lyskryds, vil vores forventning vÃ¦re at den bliver bedre til at opdage flere kategorier i hvert billede, hvilket i sidste ende vil forbedre vores metode.
Det kan desuden ogsÃ¥ diskuteres over at vores model ikke skal vÃ¦re perfekt, da et menneske ligesÃ¥ godt kan tage fejl af et objekt i et billede, hvilket Mixup ogsÃ¥ kunne tilfÃ¸je til vores model.</p>

<h2 id="prediction">Prediction</h2>
<p>Efter som vi arbejder med en Multi Category block har vi haft svÃ¦rt ved at bruge de mange af de redskaber vi er blevet undervist i sÃ¥ som confusion matrixen.
Da vores billeder primÃ¦rt kun har haft et labelt pÃ¥ har vi under det meste af udviklen haft stor bios i visse tilfÃ¦lde.
Fx har der ofte vÃ¦ret biler pÃ¥ billeder med bruger dette har fÃ¥et AIâ€™en til at vÃ¦re sikker pÃ¥ bruger frem for biler nÃ¥r der de har optrÃ¥dt i samme billede.
Tobias lavede en csv fil pÃ¥ vores lille datasÃ¦t for at migrere dette problem.
Vi har prÃ¸vet at kÃ¸re vores model mod en â€œrigtigâ€ recapture billeder fra https://www.google.com/recaptcha/api2/demo.
I stedet for at lÃ¦se html siden med Python downloade vi billedet og manuelt splitte det i de 9 billeder, hvor vi efterfÃ¸lgende putte det ind i vores AI.
Vi endte med det nedenstÃ¥ende resultat, den fangede nÃ¦sten alle cyklerne den eneste den mistede var 2,3.
Dette kunne skyldes at vi ikke bruger hele vore dataset da vi trÃ¦nede denne model men i stedet vores mindre trÃ¦ningssÃ¦t.</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_3.png â€œâ€)<br />
Image: 1, 1.jpg Prediction: [â€˜Bicycleâ€™]; Bicycle Probability: 0.9659770131111145<br />
Image: 1, 2.jpg Prediction: [â€˜Bicycleâ€™]; Bicycle Probability: 0.9705190062522888<br />
Image: 1, 3.jpg Prediction: [â€˜Bridgeâ€™]; Bicycle Probability: 4.990302880554836e-18</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_4.png â€œâ€)<br />
Image: 2, 1.jpg Prediction: [â€˜Palmâ€™]; Bicycle Probability: 0.044423192739486694<br />
Image: 2, 2.jpg Prediction: []; Bicycle Probability: 0.4902642071247101<br />
Image: 2, 3.jpg Prediction: []; Bicycle Probability: 0.20381507277488708</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_5.png â€œâ€)<br />
Image: 3, 1.jpg Prediction: [â€˜Palmâ€™]; Bicycle Probability: 0.007280856370925903<br />
Image: 3, 2.jpg Prediction: [â€˜Crosswalkâ€™]; Bicycle Probability: 0.00545891746878624<br />
Image: 3, 3.jpg Prediction: [â€˜Chimneyâ€™]; Bicycle Probability: 0.003270353190600872</p>

<p>Vi prÃ¸vede igen men en model der var trÃ¦net pÃ¥ hele vores datasÃ¦t og det sÃ¥ meget mere lovende ud.</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_3.png â€œâ€)<br />
Image: 1, 1.jpg Prediction: [â€˜Bicycleâ€™]; Bicycle Probability: 0.9953802824020386<br />
Image: 1, 2.jpg Prediction: [â€˜Bicycleâ€™]; Bicycle Probability: 0.9892680048942566<br />
Image: 1, 3.jpg Prediction: [â€˜Bridgeâ€™]; Bicycle Probability: 0.0003413711965549737</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_4.png â€œâ€)<br />
Image: 2, 1.jpg Prediction: [â€˜Palmâ€™]; Bicycle Probability: 0.0001781134633347392<br />
Image: 2, 2.jpg Prediction: [â€˜Carâ€™]; Bicycle Probability: 0.003861015196889639<br />
Image: 2, 3.jpg Prediction: [â€˜Bicycleâ€™]; Bicycle Probability: 0.8249829411506653</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_5.png â€œâ€)
Image: 3, 1.jpg Prediction: [â€˜Carâ€™]; Bicycle Probability: 0.0001316472189500928<br />
Image: 3, 2.jpg Prediction: [â€˜Crosswalkâ€™]; Bicycle Probability: 0.0038241292349994183<br />
Image: 3, 3.jpg Prediction: [â€˜Otherâ€™]; Bicycle Probability: 0.0007099288050085306</p>

<h2 id="neurale-netvÃ¦rk">Neurale netvÃ¦rk</h2>
<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>For at kunne trÃ¦ne vores model sÃ¥ den kan blive bedre til at lave forudsigelser pÃ¥ vores data, skal vi bruge en mÃ¥de at Ã¦ndre vores weights.
Dette kan gÃ¸res pÃ¥ flere mÃ¥der men en meget brugt mÃ¥de er Gradient Descent eller nÃ¦rmere Stochastic Gradient Descent.
Gradient Descent og Stochastic Gradient Descent gÃ¸r det samme og er identiske pÃ¥ nÃ¦r at man i Stochastic Gradient Descent tager lidt data og kÃ¸re igennem i stedet for alt data som i Gradient Descent.
Dette gÃ¸r ogsÃ¥ at Stochastic Gradient Descent er meget mere egnet til arbejde med BIG DATA da du ikke skal lave lige sÃ¥ mange udregnigner for hver epoch.</p>

<p>Man kan forklare hvad Gradient Descent gÃ¥r ud pÃ¥ ved at forestille sig at man skal finde den korteste ved ned af et bjerg.
Hvis man bruger traditionel Gradient Descent til at finde den korteste vej, udregner man den optimale rute pÃ¥ en gang ved at bruge data om alle de forskellige veje der er pÃ¥ bjerget.
Denne lÃ¸sning giver med garanti den korteste vej ned af bjerget, men det bliver ogsÃ¥ svÃ¦rere og svÃ¦rere at lave udregningen jo mere data man har om bjerget.
Hvis man derimod bruger Stochastic Gradient Descent til at finde vej ned af bjerget, vil man gÃ¥ ud og kigge pÃ¥ de mulige veje man kan tage pÃ¥ nuvÃ¦rende tidspunkt.
Man gÃ¥r sÃ¥ ned ad den vej der fÃ¸rer en lÃ¦ngst ned at bjerget.
Hvis vejen er meget stejl gÃ¥r man lige lidt lÃ¦nger af denne vej inden man igen kigger efter en ny vej der fÃ¸rer en lÃ¦ngere og lÃ¦ngere ned af bjerget.
Det gÃ¸r man igen og igen til man har nÃ¥et bunden.</p>

<p>Hvis vi prÃ¸ver helt simpelt at overfÃ¸re dette til vores neurale netvÃ¦rk vil det altsÃ¥ sige at vi tager en lille del af vores data og sender det igennem vores neurale netvÃ¦rk.
Vi udregner gradienten for den data og bruger den til at opdatere modellens weights.
Vi gÃ¸r sÃ¥ det hele igen med en anden lille del af vores data. Denne proces kan illustreres pÃ¥ fÃ¸lgende mÃ¥de:</p>

<p>![](/BreakingGooglesRecaptcha/images/Screenshot_6.png â€œâ€)</p>

<ol>
  <li>Init: Da vi skal have et udgangspunkt for vores weights bliver de i dette step initialiseret til tilfÃ¦ldige vÃ¦rdiger.</li>
  <li>Predict: I dette step bruger vi de tilfÃ¦ldige weights til at fÃ¥ en prediction ved at kÃ¸re en lille del af dataen igennem det neurale netvÃ¦rk.</li>
  <li>Loss: I dette step bruger vi en loss function til at mÃ¥le modellens effektivitet med de nuvÃ¦rende weights.</li>
  <li>Gradient: I dette step udregner vi vores gradient som fortÃ¦ller os hvordan Ã¦ndringen af en weight vil pÃ¥virke vores loss.</li>
  <li>Step: I dette step Ã¦ndrer vi vÃ¦rdierne af vores weights pÃ¥ baggrund af den udregnede gradient. Hvis der stadig er flere epochs tilbage, gÃ¥r vi tilbage til step 2 og gentager de efterfÃ¸lgende step.</li>
  <li>Stop: NÃ¥r der ikke er flere epochs gÃ¥r vi ikke tilbage til step 2 men stopper derimod bare.</li>
</ol>

<p>Det vigtige i denne proces er altsÃ¥ step 4 hvor vi udregner vores gradient.
Gradient er bare et fancy ord for derivative der kommer fra calculus.
SÃ¥ nÃ¥r vi siger at vi udregner vores gradient udregner vi i realiteten derivative for vores loss function.
Teknisk fortalt er derivative en funktion der beregner slope for en funktion ved en given vÃ¦rdi.
Mere simpelt fortelt fortÃ¦ller derivative os hvor stejl bjerget er pÃ¥ en given vej.</p>

<h2 id="optimering">Optimering</h2>
<h3 id="smoothing-label-smoothing">Smoothing (Label Smoothing)</h3>

<p>NÃ¥r vores model bliver trÃ¦net sÃ¥ er vores mÃ¥l i teorien at fÃ¥ et resultat der bliver 1.
Dette er resultatet pÃ¥ modellens â€œgÃ¦tâ€ ogsÃ¥ selvom den ikke er 100% sikker.
Dette betyder ogsÃ¥ at de resterende kategorier bliver 0.
Denne tilgang giver tendens til overfitting, og en model som ikke giver betydelig respons pÃ¥ dens gÃ¦t.</p>

<p>For at undgÃ¥ dette sÃ¥ kan man under opsÃ¦tningen af sin model have en loss function som benytter sig af Label Smoothing.
Label smoothing sÃ¸rger for at modellen kommer med et mere generaliserende gÃ¦t, ved at erstatte alle 1â€™erne med et tal mindre end 1 og alle 0â€™erne med (ğ / N), hvor ğ (epsilon) er et parameter oftest 0,1 og N er antallet af klasser.</p>

<p>NÃ¥r vi trÃ¦ner modellen vil vi stadig gerne have at alle labels til sammen giver resultatet 1, derfor erstatter vi 1 (det label som der bliver gÃ¦ttet pÃ¥) med fÃ¸lgende formel: (1-ğ+ğ/N).</p>

<h4 id="this-maximum-is-not-achievable-for-finite--zk--but-is-approached-if--zyzk--for-all--ky">This maximum is not achievable for finite  zk  but is approached if  zyâ‰«zk  for all  kâ‰ y</h4>
<p>Hvis man gÃ¥r over i den teoretiske forstÃ¥else af label smoothing, sÃ¥ er det en matematisk algoritme (stÃ¥ende ovenfor) som beviser at maximum ikke er opnÃ¥eligt.</p>

<p>Hvis man deler algoritmen op, sÃ¥ er y vores target og zy den activation som er tilhÃ¸rende det target.
For at komme tÃ¦t pÃ¥ 1, sÃ¥ skal zy vÃ¦re stÃ¸rre end ved alle andre â€œgÃ¦tâ€.
Dybere i algoritmen bliver det forklaret, at hvis modellen lÃ¦rer at tildele fuld sandsynlighed pÃ¥ labellet ved hver trÃ¦ning, sÃ¥ er den ikke garanteret at kunne generalisere.
Det som det betyder er at hvis zy har en hÃ¸j/stor vÃ¦rdi, sÃ¥ skal der benyttes stÃ¸rre weights og activations gennem modellen.
Dette kan resultere i at smÃ¥ Ã¦ndringen i input (f.eks. Ã¦ndring i en pixel) kan give en helt anden probability nÃ¥r der bliver gÃ¦ttet.</p>

<p>Hvis man holder algoritmen op sammen med Bounded Gradient, sÃ¥ reducerer det modellens evne til at kunne tilpasse sig.</p>

<h4 id="sidebar">Sidebar</h4>
<p>Benyttelse af Label smoothing ses ofte kun ved Single-Label.
Selve matematikken bag Label smoothing er i fÃ¸rste omgang tilegnet Single-Label, og man vil derfor skulle have en forstÃ¥else for at kunne Ã¦ndre i selve algoritmens matematik for at kunne benytte Label smoothing ved Multi-Label.</p>

<p>Det har ikke vÃ¦ret muligt at finde nogle endnu som har knÃ¦kket den matematiske kode til at benytte Label smoothing pÃ¥ Multi-Label.
Den offentlige debat lyder i hvert fald pÃ¥ at det i det nuvÃ¦rende omfang ikke er muligt at benytte Label smoothing pÃ¥ Multi-Label modeller.</p>

<h2 id="fastai">Fastai</h2>
<h3 id="dataloaders">DataLoaders</h3>
<p>DataLoaders er en meget simpel klasse, som bare wrapper/indeholder en rÃ¦kke DataLoader objekter.
Det er dog den som stÃ¥r for at levere data til modellen, og det er derfor en meget central del af FastAI.</p>

<p>Der er flere mÃ¥der man kan lave ens DataLoaders objekt pÃ¥.
FastAI indeholder en rÃ¦kke sÃ¥kaldte â€œfactory methodsâ€, som returnerer DataLoaders med prÃ¦definerede opsÃ¦tninger.
Det er en smart mÃ¥de at spare tid pÃ¥, hvis det man laver er sÃ¥ â€œalmindeligtâ€ at der allerede er en opsÃ¦tning, som passer. 
Alternativt kan man lave en custom DataLoaders. For at gÃ¸re det, bruges data block APIâ€™en.</p>

<p>NÃ¥r man laver en custom DataLoaders skal fÃ¸lgende angives, for at modellen kan trÃ¦nes:</p>
<ul>
  <li>Independent variable</li>
  <li>Dependent variable</li>
  <li>Hvordan skal data hentes</li>
  <li>Validation set</li>
  <li>Labels</li>
</ul>

<p>Derudover kan man angive item transforms, som er noget kode der bliver kÃ¸rt pÃ¥ hver entry i datasÃ¦ttet.
PÃ¥ samme mÃ¥de som med DataLoaders har FastAI en rÃ¦kke indbyggede transforms, til de mest gÃ¦ngse operationer, men det er ogsÃ¥ muligt at kode sin egen.
De indbyggede transforms inkluderer fx cropping og resizing af billeder.</p>

<p>Som et alternativ til item transforms, kan man bruge batch transforms.
Med en batch transform kÃ¸rer man i stedet noget kode pÃ¥ hele batchen i stedet for hvert individuelt billede, hvilket kan vÃ¦re hurtigere at udfÃ¸re. 
transform kan fx bruges i forbindelse med data augmentation af billeder, efter de er blevet croppet og/eller resizet til alle at vÃ¦re den samme stÃ¸rrelse.</p>
