<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Breaking Google’s Image-based reCaptcha v2 | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Breaking Google’s Image-based reCaptcha v2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Breaking Google’s Image-based reCaptcha v2." />
<meta property="og:description" content="Breaking Google’s Image-based reCaptcha v2." />
<link rel="canonical" href="https://tobbe3108.github.io/BreakingGooglesRecaptcha/markdown/2021/05/25/Breaking-Google-s-Image-based-reCaptcha-v2.html" />
<meta property="og:url" content="https://tobbe3108.github.io/BreakingGooglesRecaptcha/markdown/2021/05/25/Breaking-Google-s-Image-based-reCaptcha-v2.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-25T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Breaking Google’s Image-based reCaptcha v2.","url":"https://tobbe3108.github.io/BreakingGooglesRecaptcha/markdown/2021/05/25/Breaking-Google-s-Image-based-reCaptcha-v2.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://tobbe3108.github.io/BreakingGooglesRecaptcha/markdown/2021/05/25/Breaking-Google-s-Image-based-reCaptcha-v2.html"},"headline":"Breaking Google’s Image-based reCaptcha v2","dateModified":"2021-05-25T00:00:00-05:00","datePublished":"2021-05-25T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/BreakingGooglesRecaptcha/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://tobbe3108.github.io/BreakingGooglesRecaptcha/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/BreakingGooglesRecaptcha/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/BreakingGooglesRecaptcha/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/BreakingGooglesRecaptcha/about/">About Me</a><a class="page-link" href="/BreakingGooglesRecaptcha/search/">Search</a><a class="page-link" href="/BreakingGooglesRecaptcha/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Breaking Google’s Image-based reCaptcha v2</h1><p class="page-description">Breaking Google’s Image-based reCaptcha v2.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-25T00:00:00-05:00" itemprop="datePublished">
        May 25, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      17 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/BreakingGooglesRecaptcha/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#collect-data">Collect Data</a></li>
<li class="toc-entry toc-h2"><a href="#prepare-the-data">Prepare the data</a>
<ul>
<li class="toc-entry toc-h3"><a href="#opsætning-af-datablock">Opsætning af datablock</a></li>
<li class="toc-entry toc-h3"><a href="#dårlige-billeder">Dårlige billeder</a></li>
<li class="toc-entry toc-h3"><a href="#resultat">Resultat</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#choose-the-model">Choose the model</a></li>
<li class="toc-entry toc-h2"><a href="#train-your-machine-model">Train your machine model</a></li>
<li class="toc-entry toc-h2"><a href="#evaluation">Evaluation</a></li>
<li class="toc-entry toc-h2"><a href="#tuning--optimisation">Tuning / Optimisation</a></li>
<li class="toc-entry toc-h2"><a href="#prediction">Prediction</a></li>
<li class="toc-entry toc-h2"><a href="#neurale-netværk">Neurale netværk</a>
<ul>
<li class="toc-entry toc-h3"><a href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#optimering">Optimering</a>
<ul>
<li class="toc-entry toc-h3"><a href="#smoothing-label-smoothing">Smoothing (Label Smoothing)</a>
<ul>
<li class="toc-entry toc-h4"><a href="#sidebar">Sidebar</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#fastai">Fastai</a>
<ul>
<li class="toc-entry toc-h3"><a href="#dataloaders">DataLoaders</a></li>
</ul>
</li>
</ul><p>I dette projekt har vi valgt at vi vil forsøge at bryde Google’s reCAPTCHA v2 med et neuralt netværk.
reCAPTCHA tester den menneskelige evne til at genkende objekter på billeder, derfor er det en oplagt opgave at forsøge at løse med et neuralt netværk.
Mange virksomheder er for længst skiftet væk fra reCAPTCHA v2 til andre bedre alternativer, men man støder stadig ofte på denne reCAPTCHA.</p>

<h2 id="collect-data">
<a class="anchor" href="#collect-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Collect Data</h2>
<p>Da vi skulle finde et datasæt til at træne vores model, prøvede vi at google “recaptcha datasæt”, for at se om der allerede var nogen der havde lavet et. 3. link førte til en github repository kaldet <a href="https://github.com/brian-the-dev/recaptcha-dataset"><em>recaptcha-dataset</em></a>.
Efter et nærmere kig på billederne i datasættet, viste det sig at være et rigtig godt udgangspunkt for vores projekt.
Datasættet indeholder mere end 10.000 billeder, fordelt på 11 kategorier såsom “Bicycle”, “Crosswalk” og “Traffic Light”, som er det man ofte skal identificere i googles recaptcha.
Desuden er billederne allerede skaleret ned og beskåret til et 1:1 format, på samme måde som i recaptchaen. Datasættet er også frigivet under en MIT licens, så vi uden bekymringer kan gøre brug af det.</p>

<p>En potentiel udfordring ved datasættet er dog, at flere af billederne passer ind i flere kategorier.
Fx er der mange af billederne af “Traffic Lights” som også indeholder en “Crosswalk”.
Datasættet er dog lavet sådan, at hvert billede kun er i en kategori, og tager ikke højde for om det passer ind flere steder.</p>

<p>Efter at have downloadet datasættet valgte vi midlertidigt at trimme størstedelen af billederne væk, så der kun var en lille mængde billeder i hver kategori.
Det gjorde vi for at gøre det hurtigere at træne modellen under de indledende kode-iterationer.
Først da vi var overbeviste om at vi havde styr på koden, trænede vi modellen op imod det fulde datasæt.</p>

<h2 id="prepare-the-data">
<a class="anchor" href="#prepare-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prepare the data</h2>
<h3 id="opsætning-af-datablock">
<a class="anchor" href="#ops%C3%A6tning-af-datablock" aria-hidden="true"><span class="octicon octicon-link"></span></a>Opsætning af datablock</h3>
<p>Forberedelsen af data’en sker i vores datablock.
Vores block er en ImageBlock som er MultiCategoryBlock. Det vil altså sige at vi vil have modellen har mulighed for at kunne genkende mere end et label på billederne.</p>

<p>get_items bliver hentet gennem fastai’s get_image_files metode.
get_y bliver sat gennem egen metode get_y som sætter parent_label til billedet.
splitter bliver sat efter default parametre hvor der er en valid_pct på 0.2 og seed er 42.
item_tfms sættes til 128 og min_scale til 0.35. Dette gør at billederne har en minimumsstørrelse på 128 pixels med en scale på minimum 0.35</p>

<p>Når modellens datablock bliver instansieret sættes der ikke en batch_tfms.
I dette tilfælde betyder det at fastai automatisk får lov til at normalisere data’en udfra en enkelt batch af data’en.</p>

<h3 id="dårlige-billeder">
<a class="anchor" href="#d%C3%A5rlige-billeder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dårlige billeder</h3>
<p>I det datasæt som der er blevet fundet er der ikke blevet fjernet dårlige billeder.
Dette skyldes bl.a. at en reCaptcha er sat op til at kunne have dårlige billeder, hvor det som der bliver spurgt efter ikke nødvendigvis er i fokus eller god kvalitet.</p>
<h3 id="resultat">
<a class="anchor" href="#resultat" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resultat</h3>
<p>Som det kan ses i opsætning af datablock’en, så er de fleste værdier “default” værdier, som fastai anbefaler man benytter i første opsætning.
Dette har vi ikke ydeligere modificeret på eftersom at vores baseline model har resulteret i en accuracy på op til 94%.</p>

<h2 id="choose-the-model">
<a class="anchor" href="#choose-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Choose the model</h2>
<p>Et konventionelt neural networks network design har hver enkelte node forbundet til alle noder finding in i alle node i næste lag.
Dette gør at backprobergationen selv skal finde mønstre og segmentere datagen.
Med et Convolutional Neural Networks tvinger man netværket til at kun at kigge på mindre segmenter i stedet for selv at gøre det.
Med Convolutional Neural Networks findes der mange strategier man kan bruge til en neural Network design så som LeNet-5, AlexNet og VGG-16.
I vores projekt har vi brugt den indbygget cnn learner for multi category block.
Vi bruger Multi Category Block da det var et krav men også lader os benytte vores ressourcer bedre da vi kan tjekke hvad Recapture spørg efter samtidig med billederne bliver labeled.</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_1.png" alt=""></p>

<p>Mens vi undersøgte hvordan vi optimerer vores model valgte vi at lave et mindre datasæt end havde vi oprindeligt fandt. 
Dette gjorde at vi kunne iterere hurtigere da det tog kortere tid at træne når vi laver ændringer i vores lærings parameter.
Ulemper med denne tilgang er tendenser i vores træning sæt bliver forstærket fx de fleste modeller vi har trænet har ikke haft multi category labels på billederne men derimod kun et label.
Dette gjorde hvis der var en bro og en bil på samme billede, ville den kun vælge den ene frem for begge. Mere om dette under Prediction afsnittet.
Vi er endt med en model det svare med Multi Category Block og bruger fastai’s indbyggede cnn lerner, og for hurtig iteration har vi et mindre datasæt.</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_2.png" alt=""></p>

<h2 id="train-your-machine-model">
<a class="anchor" href="#train-your-machine-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Train your machine model</h2>
<p>Med dataene og fremgangsmåden redegjort for, er vores projekt i træningsfasen.
Her følger vi træningen igennem vores udvalgte data, og validerer gennem fasens epochs hvilke antagelser som modellen laver, og hvilke der ender ud med at være korrekte.</p>

<p>I datasamlingen fortalte vi om vores opdeling af vores datagrundlag og en test sample, som er trimmet ned for at tillade at den indledende træning forløber hurtigt.
Dette gør at vi kan opbygge vores optimale indstillinger for træningen og klargøre til det komplette datagrundlag.</p>

<p>Vi kan forsøge os med at tilføje flere kørsler i form af Epochs for at finde ud af hvilken kvantitet af træning der giver os umiddelbart den bedste nøjagtighed.
I vores trimmede dataset kører Epochs hurtigt, og giver os et indblik i hvilke resultater vi kan forvente.</p>

<p>I træningen eksperimenterer vi også med forskellige Resnets for at gøre træningen hurtigere, hvilket baner vejen for en hurtig opstart i de første Epochs, som så tilføjer de mest komplekse layers tilbage i de senere Epochs for at opbygge et grundlag for modellen og validere den til sidst.
Vi valgte at gå med Resnet50, da vi havde størst accuracy med testen undervejs og havde mest erfaring med denne mængde.</p>

<p>Det giver også mening at effektuere træningen ved hjælp af CNN learner’ens Learning Rate Find metode.
Dette giver et indblik i hvordan CNN bedst kan lærer ud fra vores data på den mest ressourceeffektive måde.</p>

<h2 id="evaluation">
<a class="anchor" href="#evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation</h2>
<p><a href="https://tobbe3108.github.io/BreakingGooglesRecaptcha/2021/05/25/Captcha-Baseline-Model.html">Baseline model notebook</a></p>

<p>For ikke at skulle træne modellen på alle 10.000 billeder hver gang da det taget lang tid har vi lavet et datasæt der tager 20 billeder fra hver kategori.
Dette datasæt har vi efterfølgende brugt til vores aktive udvikling.
Når vi har haft en ny model iteration vi gerne ville teste helt, har vi skiftet tilbage til det oprindelige datasæt med 10.000 billeder.
Vi har oplevet at dette lille datasæt har været tilstrækkeligt til at give en indikation af hvor godt modellen er.</p>

<p>I vores DataBlock har vi benyttet en RandomSplitter der udvælger 0.2 procent af billederne som vores valideringssæt.
Billederne bliver også alle kørt igennem en RandomResizedCrop der gør at vi får mere variation i vores data.</p>

<p>Vi har benyttet den indbyggede leaning rate finder (Lr_find()) til at finde den optimale learning rate for vores learner.
Lr_find() metoden køre et par iterationer med en meget lille learning rate.
For hvert mini-batch bliver der skruet en lille smule op for learning raten indtil den til sidst er meget høj.
Iterationernes loss bliver sammenlignet med den valgte learning rate og den bedste bliver valgt.</p>

<p>Da vi trænede denne model første gang med ovenstående udgangspunkt, blev vi meget overraskede over at vores model opnåede en accuracy på 94%.
Da vi testede modellen med et billede den aldrig have set før blev vi lige så overraskede.
Modellen var 99.6% sikker på at der er bro i det billede, men den var kun 0.07% sikker på at der var en bil også selv om det er meget tydeligt at se at der både er biler og en bro.
Dette betyder at vi har lavet en model der er rigtig god til at finde en enkelt ting i et billede men altså ikke flere ting som vores model skal kunne.</p>

<p>Vi har alligevel valgt at beholde denne model som vores baseline model på trods af dette da den giver et godt udgangspunkt vi kan teste vores fremtidige iterationer op imod.</p>

<h2 id="tuning--optimisation">
<a class="anchor" href="#tuning--optimisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tuning / Optimisation</h2>
<p><a href="https://tobbe3108.github.io/BreakingGooglesRecaptcha/2021/05/25/Captcha-MixUp.html">MixUp notebook</a><br>
<a href="https://tobbe3108.github.io/BreakingGooglesRecaptcha/2021/05/25/Captcha-LabelSmoothing.html">Label Smoothing v1 notebook</a><br>
<a href="https://tobbe3108.github.io/BreakingGooglesRecaptcha/2021/05/25/Captcha-Label-Smoothing-v2.html">Label Smoothing v2 notebook</a><br>
<a href="https://tobbe3108.github.io/BreakingGooglesRecaptcha/2021/05/25/Captcha-Label-Correction.html">Label Correction notebook</a></p>

<p>Med vores nyoprettet baseline model, justerer vi vores forskellige parameter for at opnå det bedste resultat.
For at sikre imod at vores model blev for sikker og “overfitted”, brugte vi Label Smoothing undervejs i træningen for at gøre modellen mere modtagelig over for et mere nuanceret gæt i sidste ende, sådan at modellen kan være bedre til at gætte på andre mulige resultater i de billeder, som vi skal genkende.
Dette er rigtig nyttigt, til billeder som indeholder fx biler og broer, hvor at begge klassifikationer er korrekte, og gør at vi kan validere om modellen tror at en bestemt kategori findes på billedet.
Når reCAPTCHA så spørger efter alle bjergene på en række billeder, så er det ikke et problem at der er en bil på nogle af billederne.
Vores model kan stadig vælge at kigge efter den specifikke kategori, beregner hvorvidt der er en sandsynlighed for at et billede indeholder et bjerg og kan på den måde stadig svare rigtig.
Dette kræver dog bare at vi specificerer et threshold for hvor lav sandsynlighed der skulle accepteres.</p>

<p>Til sidst i vores forsøg på at optimere vores model, tilføjede vi en Mixup Callback metode til vores learner for at give bedre nøjagtighed på vores billedgenkendelse.
Med Mixup kan vi blande vores billeder sammen for at billederne skal genkendes ind over hinanden i et forsøg på at gøre learneren åben over for tvetydighed i vores data.
Når vores learner støder på et billede der er opbygget af 20 % biler og 80 % lyskryds, vil vores forventning være at den bliver bedre til at opdage flere kategorier i hvert billede, hvilket i sidste ende vil forbedre vores metode.
Det kan desuden også diskuteres over at vores model ikke skal være perfekt, da et menneske ligeså godt kan tage fejl af et objekt i et billede, hvilket Mixup også kunne tilføje til vores model.</p>

<h2 id="prediction">
<a class="anchor" href="#prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prediction</h2>
<p>Efter som vi arbejder med en Multi Category block har vi haft svært ved at bruge de mange af de redskaber vi er blevet undervist i så som confusion matrixen.
Da vores billeder primært kun har haft et labelt på har vi under det meste af udviklen haft stor bios i visse tilfælde.
Fx har der ofte været biler på billeder med bruger dette har fået AI’en til at være sikker på bruger frem for biler når der de har optrådt i samme billede.
Tobias lavede en csv fil på vores lille datasæt for at migrere dette problem.
Vi har prøvet at køre vores model mod en “rigtig” recapture billeder fra https://www.google.com/recaptcha/api2/demo.
I stedet for at læse html siden med Python downloade vi billedet og manuelt splitte det i de 9 billeder, hvor vi efterfølgende putte det ind i vores AI.
Vi endte med det nedenstående resultat, den fangede næsten alle cyklerne den eneste den mistede var 2,3.
Dette kunne skyldes at vi ikke bruger hele vore dataset da vi trænede denne model men i stedet vores mindre træningssæt.</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_3.png" alt=""><br>
Image: 1, 1.jpg Prediction: [‘Bicycle’]; Bicycle Probability: 0.9659770131111145<br>
Image: 1, 2.jpg Prediction: [‘Bicycle’]; Bicycle Probability: 0.9705190062522888<br>
Image: 1, 3.jpg Prediction: [‘Bridge’]; Bicycle Probability: 4.990302880554836e-18</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_4.png" alt=""><br>
Image: 2, 1.jpg Prediction: [‘Palm’]; Bicycle Probability: 0.044423192739486694<br>
Image: 2, 2.jpg Prediction: []; Bicycle Probability: 0.4902642071247101<br>
Image: 2, 3.jpg Prediction: []; Bicycle Probability: 0.20381507277488708</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_5.png" alt=""><br>
Image: 3, 1.jpg Prediction: [‘Palm’]; Bicycle Probability: 0.007280856370925903<br>
Image: 3, 2.jpg Prediction: [‘Crosswalk’]; Bicycle Probability: 0.00545891746878624<br>
Image: 3, 3.jpg Prediction: [‘Chimney’]; Bicycle Probability: 0.003270353190600872</p>

<p>Vi prøvede igen men en model der var trænet på hele vores datasæt og det så meget mere lovende ud.</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_3.png" alt=""><br>
Image: 1, 1.jpg Prediction: [‘Bicycle’]; Bicycle Probability: 0.9953802824020386<br>
Image: 1, 2.jpg Prediction: [‘Bicycle’]; Bicycle Probability: 0.9892680048942566<br>
Image: 1, 3.jpg Prediction: [‘Bridge’]; Bicycle Probability: 0.0003413711965549737</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_4.png" alt=""><br>
Image: 2, 1.jpg Prediction: [‘Palm’]; Bicycle Probability: 0.0001781134633347392<br>
Image: 2, 2.jpg Prediction: [‘Car’]; Bicycle Probability: 0.003861015196889639<br>
Image: 2, 3.jpg Prediction: [‘Bicycle’]; Bicycle Probability: 0.8249829411506653</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_5.png" alt="">
Image: 3, 1.jpg Prediction: [‘Car’]; Bicycle Probability: 0.0001316472189500928<br>
Image: 3, 2.jpg Prediction: [‘Crosswalk’]; Bicycle Probability: 0.0038241292349994183<br>
Image: 3, 3.jpg Prediction: [‘Other’]; Bicycle Probability: 0.0007099288050085306</p>

<h2 id="neurale-netværk">
<a class="anchor" href="#neurale-netv%C3%A6rk" aria-hidden="true"><span class="octicon octicon-link"></span></a>Neurale netværk</h2>
<h3 id="stochastic-gradient-descent">
<a class="anchor" href="#stochastic-gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic Gradient Descent</h3>
<p>For at kunne træne vores model så den kan blive bedre til at lave forudsigelser på vores data, skal vi bruge en måde at ændre vores weights.
Dette kan gøres på flere måder men en meget brugt måde er Gradient Descent eller nærmere Stochastic Gradient Descent.
Gradient Descent og Stochastic Gradient Descent gør det samme og er identiske på nær at man i Stochastic Gradient Descent tager lidt data og køre igennem i stedet for alt data som i Gradient Descent.
Dette gør også at Stochastic Gradient Descent er meget mere egnet til arbejde med BIG DATA da du ikke skal lave lige så mange udregnigner for hver epoch.</p>

<p>Man kan forklare hvad Gradient Descent går ud på ved at forestille sig at man skal finde den korteste ved ned af et bjerg.
Hvis man bruger traditionel Gradient Descent til at finde den korteste vej, udregner man den optimale rute på en gang ved at bruge data om alle de forskellige veje der er på bjerget.
Denne løsning giver med garanti den korteste vej ned af bjerget, men det bliver også sværere og sværere at lave udregningen jo mere data man har om bjerget.
Hvis man derimod bruger Stochastic Gradient Descent til at finde vej ned af bjerget, vil man gå ud og kigge på de mulige veje man kan tage på nuværende tidspunkt.
Man går så ned ad den vej der fører en længst ned at bjerget.
Hvis vejen er meget stejl går man lige lidt længer af denne vej inden man igen kigger efter en ny vej der fører en længere og længere ned af bjerget.
Det gør man igen og igen til man har nået bunden.</p>

<p>Hvis vi prøver helt simpelt at overføre dette til vores neurale netværk vil det altså sige at vi tager en lille del af vores data og sender det igennem vores neurale netværk.
Vi udregner gradienten for den data og bruger den til at opdatere modellens weights.
Vi gør så det hele igen med en anden lille del af vores data. Denne proces kan illustreres på følgende måde:</p>

<p><img src="https://tobbe3108.github.io/BreakingGooglesRecaptcha/images/Screenshot_6.png" alt=""></p>

<ol>
  <li>Init: Da vi skal have et udgangspunkt for vores weights bliver de i dette step initialiseret til tilfældige værdiger.</li>
  <li>Predict: I dette step bruger vi de tilfældige weights til at få en prediction ved at køre en lille del af dataen igennem det neurale netværk.</li>
  <li>Loss: I dette step bruger vi en loss function til at måle modellens effektivitet med de nuværende weights.</li>
  <li>Gradient: I dette step udregner vi vores gradient som fortæller os hvordan ændringen af en weight vil påvirke vores loss.</li>
  <li>Step: I dette step ændrer vi værdierne af vores weights på baggrund af den udregnede gradient. Hvis der stadig er flere epochs tilbage, går vi tilbage til step 2 og gentager de efterfølgende step.</li>
  <li>Stop: Når der ikke er flere epochs går vi ikke tilbage til step 2 men stopper derimod bare.</li>
</ol>

<p>Det vigtige i denne proces er altså step 4 hvor vi udregner vores gradient.
Gradient er bare et fancy ord for derivative der kommer fra calculus.
Så når vi siger at vi udregner vores gradient udregner vi i realiteten derivative for vores loss function.
Teknisk fortalt er derivative en funktion der beregner slope for en funktion ved en given værdi.
Mere simpelt fortelt fortæller derivative os hvor stejl bjerget er på en given vej.</p>

<h2 id="optimering">
<a class="anchor" href="#optimering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimering</h2>
<h3 id="smoothing-label-smoothing">
<a class="anchor" href="#smoothing-label-smoothing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Smoothing (Label Smoothing)</h3>

<p>Når vores model bliver trænet så er vores mål i teorien at få et resultat der bliver 1.
Dette er resultatet på modellens “gæt” også selvom den ikke er 100% sikker.
Dette betyder også at de resterende kategorier bliver 0.
Denne tilgang giver tendens til overfitting, og en model som ikke giver betydelig respons på dens gæt.</p>

<p>For at undgå dette så kan man under opsætningen af sin model have en loss function som benytter sig af Label Smoothing.
Label smoothing sørger for at modellen kommer med et mere generaliserende gæt, ved at erstatte alle 1’erne med et tal mindre end 1 og alle 0’erne med (𝝐 / N), hvor 𝝐 (epsilon) er et parameter oftest 0,1 og N er antallet af klasser.</p>

<p>Når vi træner modellen vil vi stadig gerne have at alle labels til sammen giver resultatet 1, derfor erstatter vi 1 (det label som der bliver gættet på) med følgende formel: (1-𝝐+𝝐/N).</p>

<p><strong>This maximum is not achievable for finite  zk  but is approached if  zy≫zk  for all  k≠y</strong></p>

<p>Hvis man går over i den teoretiske forståelse af label smoothing, så er det en matematisk algoritme (stående ovenfor) som beviser at maximum ikke er opnåeligt.</p>

<p>Hvis man deler algoritmen op, så er y vores target og zy den activation som er tilhørende det target.
For at komme tæt på 1, så skal zy være større end ved alle andre “gæt”.
Dybere i algoritmen bliver det forklaret, at hvis modellen lærer at tildele fuld sandsynlighed på labellet ved hver træning, så er den ikke garanteret at kunne generalisere.
Det som det betyder er at hvis zy har en høj/stor værdi, så skal der benyttes større weights og activations gennem modellen.
Dette kan resultere i at små ændringen i input (f.eks. ændring i en pixel) kan give en helt anden probability når der bliver gættet.</p>

<p>Hvis man holder algoritmen op sammen med Bounded Gradient, så reducerer det modellens evne til at kunne tilpasse sig.</p>

<h4 id="sidebar">
<a class="anchor" href="#sidebar" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sidebar</h4>
<p>Benyttelse af Label smoothing ses ofte kun ved Single-Label.
Selve matematikken bag Label smoothing er i første omgang tilegnet Single-Label, og man vil derfor skulle have en forståelse for at kunne ændre i selve algoritmens matematik for at kunne benytte Label smoothing ved Multi-Label.</p>

<p>Det har ikke været muligt at finde nogle endnu som har knækket den matematiske kode til at benytte Label smoothing på Multi-Label.
Den offentlige debat lyder i hvert fald på at det i det nuværende omfang ikke er muligt at benytte Label smoothing på Multi-Label modeller.</p>

<h2 id="fastai">
<a class="anchor" href="#fastai" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fastai</h2>
<h3 id="dataloaders">
<a class="anchor" href="#dataloaders" aria-hidden="true"><span class="octicon octicon-link"></span></a>DataLoaders</h3>
<p>DataLoaders er en meget simpel klasse, som bare wrapper/indeholder en række DataLoader objekter.
Det er dog den som står for at levere data til modellen, og det er derfor en meget central del af FastAI.</p>

<p>Der er flere måder man kan lave ens DataLoaders objekt på.
FastAI indeholder en række såkaldte “factory methods”, som returnerer DataLoaders med prædefinerede opsætninger.
Det er en smart måde at spare tid på, hvis det man laver er så “almindeligt” at der allerede er en opsætning, som passer. 
Alternativt kan man lave en custom DataLoaders. For at gøre det, bruges data block API’en.</p>

<p>Når man laver en custom DataLoaders skal følgende angives, for at modellen kan trænes:</p>
<ul>
  <li>Independent variable</li>
  <li>Dependent variable</li>
  <li>Hvordan skal data hentes</li>
  <li>Validation set</li>
  <li>Labels</li>
</ul>

<p>Derudover kan man angive item transforms, som er noget kode der bliver kørt på hver entry i datasættet.
På samme måde som med DataLoaders har FastAI en række indbyggede transforms, til de mest gængse operationer, men det er også muligt at kode sin egen.
De indbyggede transforms inkluderer fx cropping og resizing af billeder.</p>

<p>Som et alternativ til item transforms, kan man bruge batch transforms.
Med en batch transform kører man i stedet noget kode på hele batchen i stedet for hvert individuelt billede, hvilket kan være hurtigere at udføre. 
transform kan fx bruges i forbindelse med data augmentation af billeder, efter de er blevet croppet og/eller resizet til alle at være den samme størrelse.</p>

  </div><a class="u-url" href="/BreakingGooglesRecaptcha/markdown/2021/05/25/Breaking-Google-s-Image-based-reCaptcha-v2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/BreakingGooglesRecaptcha/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/BreakingGooglesRecaptcha/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/BreakingGooglesRecaptcha/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/BreakingGooglesRecaptcha/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/BreakingGooglesRecaptcha/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
